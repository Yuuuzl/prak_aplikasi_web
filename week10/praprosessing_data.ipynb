{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          clean_text  category\n",
      "0  when modi promised “minimum government maximum...      -1.0\n",
      "1  talk all the nonsense and continue all the dra...       0.0\n",
      "2  what did just say vote for modi  welcome bjp t...       1.0\n",
      "3  asking his supporters prefix chowkidar their n...       1.0\n",
      "4  answer who among these the most powerful world...       1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Twitter data\n",
    "file_path = 'Twitter_Data.csv'  # File path from your upload\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to understand the data\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          clean_text  category\n",
      "0  when modi promised “minimum government maximum...      -1.0\n",
      "1  talk all the nonsense and continue all the dra...       0.0\n",
      "2  what did just say vote for modi  welcome bjp t...       1.0\n",
      "3  asking his supporters prefix chowkidar their n...       1.0\n",
      "4  answer who among these the most powerful world...       1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wahyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wahyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\wahyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          clean_text  \\\n",
      "0  when modi promised “minimum government maximum...   \n",
      "1  talk all the nonsense and continue all the dra...   \n",
      "2  what did just say vote for modi  welcome bjp t...   \n",
      "3  asking his supporters prefix chowkidar their n...   \n",
      "4  answer who among these the most powerful world...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  when modi promised minimum government maximum ...  \n",
      "1  talk all the nonsense and continue all the dra...  \n",
      "2  what did just say vote for modi welcome bjp to...  \n",
      "3  asking his supporters prefix chowkidar their n...  \n",
      "4  answer who among these the most powerful world...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Twitter data\n",
    "file_path = 'Twitter_Data.csv'  # File path from your upload\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to understand the data\n",
    "print(df.head())\n",
    "\n",
    "# Download NLTK stop words and tokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Get English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Check if the text is a string\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove non-ASCII characters (like â€œ, â€)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "        \n",
    "        # Remove punctuation and special characters\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    else:\n",
    "        return ''  # Return an empty string for non-string values (like NaN)\n",
    "\n",
    "# Apply the cleaning function to the 'clean_text' column\n",
    "df['cleaned_text'] = df['clean_text'].apply(clean_text)\n",
    "\n",
    "# Display the cleaned data\n",
    "print(df[['clean_text', 'cleaned_text']].head())\n",
    "\n",
    "# Save cleaned data to CSV\n",
    "df.to_csv('Cleaned_Twitter_Data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wahyuenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
